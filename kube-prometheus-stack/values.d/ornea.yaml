# helm upgrade kube-prometheus-stack -n clusterwide prometheus-community/kube-prometheus-stack -f kube-prometheus-stack/values.d/ornea.yaml --debug --kube-context gke_evident-gecko-364317_europe-west8-b_ornea

defaultRules:
  create: true
#  rules:
#    alertmanager: false
#    etcd: false
#    configReloaders: false
#    general: false
#    k8s: false
#    kubeApiserverAvailability: false
#    kubeApiserverBurnrate: false
#    kubeApiserverHistogram: false
#    kubeApiserverSlos: false
#    kubeControllerManager: false
#    kubelet: false
#    kubeProxy: false
#    kubePrometheusGeneral: false
#    kubePrometheusNodeRecording: false
#    kubernetesApps: false
#    kubernetesResources: false
#    kubernetesStorage: false
#    kubernetesSystem: false
#    kubeSchedulerAlerting: false
#    kubeSchedulerRecording: false
#    kubeStateMetrics: false
#    network: false
#    node: false
#    nodeExporterAlerting: false
#    nodeExporterRecording: false
#    prometheus: false
#    prometheusOperator: false

alertmanager:
  enabled: false

grafana:
  enabled: false

kubeApiServer:
  enabled: false

kubelet:
  enabled: false

kubeControllerManager:
  enabled: false

kube-state-metrics:
  prometheus:
    monitor:
      interval: 60s
      scrapeTimeout: 60s

coreDns:
  enabled: false

kubeEtcd:
  enabled: false

kubeScheduler:
  enabled: false

kubeProxy:
  enabled: false

kubeStateMetrics:
  enabled: false

nodeExporter:
  enabled: false

prometheusOperator:
  enabled: true

  admissionWebhooks:
    enabled: true

  namespaces:
    releaseNamespace: true
    additional:
      - default
      - clusterwide
      - cert-manager
      - circleci
      - web
      - chat
      - db

  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
  ##
  clusterDomain: "cluster.local"

  networkPolicy:
    ## Enable creation of NetworkPolicy resources.
    ##
    enabled: false

  ## Define Log Format
  # Use logfmt (default) or json logging
  logFormat: json

  ## Decrease log verbosity to errors only
  # logLevel: error

  kubeletService:
    enabled: false

  resources:
   requests:
     cpu: 10m
     memory: 30Mi
   limits:
     cpu: 50m
     memory: 300Mi


  ## Prometheus-config-reloader
  ##
  prometheusConfigReloader:
    resources:
      requests:
        cpu: 10m
        memory: 10Mi
      limits:
        cpu: 20m
        memory: 25Mi

  serviceMonitor:
    selfMonitor: false

prometheus:
  enabled: true

  ## ExtraSecret can be used to store various data in an extra secret
  ## (use it for example to store hashed basic auth credentials)
  #extraSecret:
  #  name: "grafana-net"
  #  data:
  #    auth: |
  #      foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0

  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "500"
      nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.web.svc.cluster.local/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://auth.ornea.kix.co.il/oauth2/start?rd=%2Fgo%2Fprometheus$escaped_request_uri"
      nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email
      nginx.ingress.kubernetes.io/configuration-snippet: |
        auth_request_set $remote_email $upstream_http_x_auth_request_email;
        add_header X-Auth-Request-Email $remote_email;
    labels: {}
    hosts:
      - prometheus.ornea.kix.co.il
    paths:
      - /
    tls:
      - hosts:
          - prometheus.ornea.kix.co.il
        secretName: prometheus-tls

  prometheusSpec:
    scrapeInterval: "120s"
    scrapeTimeout: "120s"

    ## Alertmanagers to which alerts will be sent
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints
    ##
    ## Default configuration will connect to the alertmanager deployed as part of this release
    ##
    alertingEndpoints: []
#      - name: ""
#        namespace: ""
#        port: http
#        scheme: http
#        pathPrefix: ""
#        tlsConfig: {}
#        bearerTokenFile: ""
#        apiVersion: v2

    ruleNamespaceSelector:
      matchLabels: {}

    serviceMonitorSelector: # {} ***
      matchLabels: {}

    serviceMonitorNamespaceSelector:
      matchLabels: {}

    podMonitorSelector: # {} ***
      matchLabels: {}

    podMonitorNamespaceSelector:
      matchLabels: {}

    probeSelector: # {} ***
      matchLabels: {}

    probeNamespaceSelector:
      matchLabels: {}

    logLevel: info
    logFormat: json

    remoteWrite:
      - url: https://prometheus-prod-05-gb-south-0.grafana.net/api/prom/push
        basicAuth:
          username:
            name: prometheus-remote-push
            key: username
          password:
            name: prometheus-remote-push
            key: password
    # - url: http://remote1/push
    ## additionalRemoteWrite is appended to remoteWrite
    additionalRemoteWrite: []

    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
    remoteWriteDashboards: false

    ## Resource limits & requests
    ##
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        cpu: 400m
        memory: 1200Mi

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    storageSpec: {}
    ## Using PersistentVolumeClaim
    ##
    #  volumeClaimTemplate:
    #    spec:
    #      storageClassName: gluster
    #      accessModes: ["ReadWriteOnce"]
    #      resources:
    #        requests:
    #          storage: 50Gi
    #    selector: {}

    ## Using tmpfs volume
    ##
    #  emptyDir:
    #    medium: Memory

    additionalScrapeConfigs: []
    # - job_name: kube-etcd
    #   kubernetes_sd_configs:
    #     - role: node
    #   scheme: https
    #   tls_config:
    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    #   relabel_configs:
    #   - action: labelmap
    #     regex: __meta_kubernetes_node_label_(.+)
    #   - source_labels: [__address__]
    #     action: replace
    #     targetLabel: __address__
    #     regex: ([^:;]+):(\d+)
    #     replacement: ${1}:2379
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: keep
    #     regex: .*mst.*
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: replace
    #     targetLabel: node
    #     regex: (.*)
    #     replacement: ${1}
    #   metric_relabel_configs:
    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
    #     action: labeldrop
    #
    ## If scrape config contains a repetitive section, you may want to use a template.
    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
    # additionalScrapeConfigs: |
    #  - job_name: "node-exporter"
    #    gce_sd_configs:
    #    {{range $zone := .Values.gcp_zones}}
    #    - project: "project1"
    #      zone: "{{$zone}}"
    #      port: 9100
    #    {{end}}
    #    relabel_configs:
    #    ...


    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
    ##
    additionalAlertManagerConfigs: []
    # - consul_sd_configs:
    #   - server: consul.dev.test:8500
    #     scheme: http
    #     datacenter: dev
    #     tag_separator: ','
    #     services:
    #       - metrics-prometheus-alertmanager


    retention: 1h
  serviceMonitor:
    selfMonitor: false

thanosRuler:
  enabled: false
